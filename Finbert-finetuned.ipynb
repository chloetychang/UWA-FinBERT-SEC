{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/chloetychang/UWA-FinBert-SEC/blob/main/Finbert-finetuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIth-FRgTAgc"
   },
   "source": [
    "# Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lsEZ2qKf0Ky"
   },
   "source": [
    "First, create the environment from the `environment.yml`:\n",
    "```bash\n",
    "conda env create -f environment.yml         # created using `conda env export > environment.yml`\n",
    "```\n",
    "By running this command in terminal...\n",
    "- A new Conda environmnet gets created (with the name defined in the YAML)\n",
    "- The packages as specified gets installed (both Conda and pip dependencies)\n",
    "- Compatible versions for your OS gets automatically resolved\n",
    "\n",
    "Once the packages are installed, run:\n",
    "```bash\n",
    "conda activate finbert-sec-env\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVsfLhxlVGZ1"
   },
   "outputs": [],
   "source": [
    "# Import different python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CMfUHYpf0Kz"
   },
   "source": [
    "To remove Jupyter Notebook output when committing to GitHub:\n",
    "```bash\n",
    "nbstripout --install\n",
    "pip install nbstripout\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwLLWbiC9XFd"
   },
   "outputs": [],
   "source": [
    "# !pip install chardet\n",
    "## chardet is used for detecting file encoding before reading raw bytes.\n",
    "## No need for chardet — Parquet stores text as UTF-8 internally.\n",
    "\n",
    "# import chardet\n",
    "# result = chardet.detect(parquet[\"text_pr\"])\n",
    "# encoding = result['encoding']\n",
    "\n",
    "## To find what encoding type of data\n",
    "# encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z5L4Dj1C5d1"
   },
   "source": [
    "# Load SEC Press Releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "W6AdqswNU5UF",
    "outputId": "6235b54b-a265-4ecc-95a7-77cceafd842b"
   },
   "outputs": [],
   "source": [
    "# Read Parquet File `sec.parquet`\n",
    "parquet = pd.read_parquet('sec.parquet')          # In Google Colab, change to \"/content/sec.parquet\"\n",
    "parquet.head()\n",
    "\n",
    "# Focus on SEC Press Releases - `text_pr` column\n",
    "parquet_text = parquet[\"text_pr\"]\n",
    "parquet_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeeYvcCclo3d"
   },
   "outputs": [],
   "source": [
    "# Get basic info of the dataframe\n",
    "parquet.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03l5gT51-YgN"
   },
   "source": [
    "# Load Finbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "2afa86609cef40e3b8cc5a6ef7af466d",
      "10b893d5864f4c4fbb4b8bd9c7574d14",
      "bd21387049744ce387b0174c05d8c649",
      "ec9d1b667db04275b2ca17cc57a12815",
      "e7836c62dc674bc6b2bd967fb6c84051",
      "9695ac62d82949ffbbede248fe70220a",
      "7028ed246a2844799ff489b954ccf6fa",
      "3ad5851996ce4fbcb9df6c1eb182d4e4",
      "ce1059dbc212494db952d72b03c2c844",
      "d481f1881cc348dd8447f4ffe7338b01",
      "2f89c77eda3c43a8a49330d7c98da2ae",
      "75c0c9bb74dd41a6b10e99888df7a83a",
      "977a59f47b3e42afa6829076be5d2c9b",
      "73173bcc79a3496cab48cee45a4befba",
      "e7e3b89c4d26474c9920e31563a5f564",
      "5e465b8873484e7fb679de8eabb75acf",
      "69cff6f1acbd41f187dcf8302a652063",
      "b18f4ae3053c412fa880dbfac2ec9147",
      "13ef2cc0387848a093d86382a6b9082f",
      "030ec18302624b28a3a89923eced58a3",
      "f857fcf403cf40708f713a8f24d675c1",
      "2b6401d3056f40128975a803bdab6c54",
      "70037b0769224f008d4bf6787852ed03",
      "8bbb8b3383754c48925ab6239f23d98f",
      "54c428baf4e44b23b1ff9c7c536e8cb1",
      "855b3c20994f450bb775b0b0da6b0135",
      "907e4f6989f14876acd8971472500de7",
      "68fd7511ccab4a7da5a970df2e3fa68a",
      "34ef42de04a44f5ca39db865b98d9c36",
      "35046fff21e2414d9890c540d9688e54",
      "e5a84816d10d4b4898f59285dc066b54",
      "251dc20f614f4e708492384915be88b7",
      "a10da7e58c97482b8e550f6b9fa4c75b",
      "ecdfbdf26cc042fa9a60e516421412ef",
      "8730f77cfa7d4cb990f9b63be9cf0bae",
      "5fd1aa32d1bd48618d7b6b9655702317",
      "a29c733ded9b4ef79ca89476e76eded8",
      "64792ade72504758bb157eb9481c8866",
      "1f1bd10999074d4a9d9d78f692c9c287",
      "c03c3b37640f4b13a6e2db12d7dda5e7",
      "8e7c80ebb2f84d4e996c7283dcfd0d22",
      "116a1777ec2c47638563f6bb30526d9b",
      "95aceec1d7b14dcfadb0ecef51701fd3",
      "e03c46b472d64b589f10ed5b55d5f8e8"
     ]
    },
    "id": "nCfMQTuo-K32",
    "outputId": "10bd96d4-44b2-45d9-d29b-154a0ce216b0"
   },
   "outputs": [],
   "source": [
    "# Colab command to install the library \"transformers\": !pip install transformers==4.28.\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "finbert = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALYVVwywTILb"
   },
   "source": [
    "## Using spacy to split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cV-OZ4ZKRn92",
    "outputId": "db93775c-5860-4bc8-c532-61e1324fd1d8"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentences = []\n",
    "for text in tqdm(parquet_text):\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        sentences.append(sent.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvFM1xaVfZhf"
   },
   "source": [
    "# Predict the original dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLssnjc2DOol",
    "outputId": "b10f1b8a-75ff-4c46-87d3-f3b42a3f57bc"
   },
   "outputs": [],
   "source": [
    "nlp(parquet_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "gb8aqB43_x2P",
    "outputId": "2d375c7d-f9fa-4028-f242-dac5c833b14e"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for sentence in tqdm(sentences):\n",
    "    doc = finbert(sentence)\n",
    "    label = doc[0]['label']\n",
    "    score = doc[0]['score']\n",
    "    results.append({'sentence': sentence, 'label': label, 'score': score})\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hx-ZmSwAEKBu"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II75DqQOd3HR"
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "# Potential use in the future, but a csv with sentences grouped by original dataframe entry will be generated later in the notebook.\n",
    "# df.to_csv('SEC_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skxuKkkOAFYk"
   },
   "source": [
    "# Text Simplification (sentiment focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKLbZe0KTWbV"
   },
   "source": [
    "**Note on Text Simplification**\n",
    "\n",
    "SEC enforcement press releases differ fundamentally from narrative policy texts like FOMC statements. They are shorter, more direct, and written in a formal legal register, typically featuring constructions such as “The SEC today charged…”, “Without admitting or denying…”, or “The firm agreed to pay…”.\n",
    "\n",
    "These sentences rarely use hedged or contrastive connectors (e.g., “although”, “but”, “while”) in ways that affect sentiment. Applying preprocessing steps like `remove_comma()` or `sentiment_focus()` could inadvertently strip legally significant clauses such as names, charges, or disclaimers, which contains essential context.\n",
    "\n",
    "As a result, such preprocessing would not enhance sentiment extraction accuracy and could, in fact, distort the legal tone central to SEC communications.\n",
    "\n",
    "\n",
    "---\n",
    "**Note on Sentiment Focus - Why `sentiment_focus()` Was Not Applied to SEC Press Releases**\n",
    "\n",
    "The `sentiment_focus()` filtering step, used in FOMC sentiment analysis, was omitted for SEC enforcement press releases. Unlike policy statements, nearly every sentence in an enforcement release carries informational or evaluative weight. Clauses that appear factual, such as “The SEC today charged…”, “Without admitting or denying…”, or “The firm agreed to pay…”, are legally meaningful and influence perceived tone and market response. Filtering them out could remove crucial signals about enforcement severity, cooperation, or settlement outcomes.\n",
    "\n",
    "Therefore, all sentences were retained, and sentiment was evaluated at the sentence level before aggregation, preserving the full legal and contextual nuance of SEC communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfpVHopCru3F"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Function to remove comma before root or conjunction using SpaCy.\n",
    "# This function is not used in the current implementation but can be useful for preprocessing sentences - code commented out for potential future reference.\n",
    "'''\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def remove_comma(sentence):\n",
    "#     doc = nlp(sentence)\n",
    "#     indices = []\n",
    "#     for i, token in enumerate(doc):\n",
    "#         if token.dep_ == \"punct\":\n",
    "#             try:\n",
    "#                 next_token = doc[i+1]\n",
    "#                 if next_token.dep_ == \"ROOT\" or next_token.dep_ == \"conj\":\n",
    "#                     indices.append(i)\n",
    "#             except IndexError:\n",
    "#                 pass\n",
    "#     if not indices:\n",
    "#         return sentence\n",
    "#     else:\n",
    "#         parts = []\n",
    "#         last_idx = 0\n",
    "#         for idx in indices:\n",
    "#             parts.append(doc[last_idx:idx].text.strip())\n",
    "\n",
    "#             last_idx = idx+1\n",
    "#         parts.append(doc[last_idx:].text.strip())\n",
    "#         return \" \".join(parts)\n",
    "\n",
    "# # Example of remove_comma\n",
    "# remove_comma(\"The personal saving rate--while still slightly negative,moved up in October.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DR75o9Iyjts1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to identify sentiment focus in a sentence using SpaCy.\n",
    "This function is not used in the current implementation but can be useful for identifying the main sentiment-bearing part of a sentence.\n",
    "Ccode commented out for potential future reference.\n",
    "'''\n",
    "\n",
    "# def sentiment_focus(sentence):\n",
    "#     doc = nlp(sentence)\n",
    "#     focus = \"\"\n",
    "#     focus_changed = 1\n",
    "#     for token in doc[:-1]:\n",
    "#       if token.lower_ == \"but\":\n",
    "#           focus = doc[token.i + 1:]\n",
    "#           return str(focus).strip(),focus_changed\n",
    "\n",
    "#     for sent in doc.sents:\n",
    "#         sent_tokens = [token for token in sent]\n",
    "#         for token in sent_tokens:\n",
    "#             if token.lower_ == \"although\" or token.lower_ == \"though\":\n",
    "#                 try:\n",
    "#                     comma_index_back = [token1.i for token1 in doc[token.i:] if token1.text == ','][0]\n",
    "#                 except IndexError:\n",
    "#                     try:\n",
    "#                       comma_index_front = [token1.i for token1 in doc[:token.i] if token1.text == ','][-1]\n",
    "#                     except IndexError:\n",
    "#                       return str(doc).strip(),focus_changed\n",
    "#                     focus = doc[:comma_index_front].text\n",
    "#                     return str(focus).strip(),focus_changed\n",
    "#                 try:\n",
    "#                       comma_index_front = [token1.i for token1 in doc[:token.i] if token1.text == ','][-1]\n",
    "#                 except IndexError:\n",
    "#                   focus = doc[comma_index_back+1:].text\n",
    "#                   return str(focus).strip(),focus_changed\n",
    "#                 focus = doc[:comma_index_front].text+doc[comma_index_back:].text\n",
    "#                 return str(focus).strip(),focus_changed\n",
    "\n",
    "#     if doc[0].lower_ == \"while\":\n",
    "#       try:\n",
    "#         comma_index_back1 = [token2.i for token2 in doc if token2.text == ','][0]\n",
    "#       except IndexError:\n",
    "#         return str(doc).strip(),focus_changed\n",
    "#       focus = doc[comma_index_back1+1:].text\n",
    "#       return str(focus).strip(),focus_changed\n",
    "\n",
    "#     focus_changed = 0\n",
    "#     return str(doc).strip(),focus_changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2EIvvLRjssI"
   },
   "source": [
    "# Fine-tuning FinBERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehFEHBfpIBZ_"
   },
   "source": [
    "Import libraries needed in fine tuning FinBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljKvG7SKXCXQ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Colab Commmands to install necessary libraries:\n",
    "\n",
    "!pip install transformers==4.28.1\n",
    "!pip install datasets\n",
    "'''\n",
    "\n",
    "from transformers import BertTokenizer, Trainer, BertForSequenceClassification, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import transformers\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78PHuzrmnk1I"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyTqSxnFf0K4"
   },
   "outputs": [],
   "source": [
    "# Split CSV file into training, validation, and testing sets\n",
    "\n",
    "# Filter file to only include sentences with a score above 0.95\n",
    "df_filtered = df[df['score'] > 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKPjUDyRnyr2"
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVkAs1xjwMPt"
   },
   "outputs": [],
   "source": [
    "# Copy the filtered dataframe to a new dataframe\n",
    "df = df_filtered[['sentence', 'label']].copy()\n",
    "\n",
    "# Convert text labels to numeric values\n",
    "df['label'] = df['label'].replace({'Neutral': 0, 'Positive': 1, 'Negative': 2})\n",
    "\n",
    "# Dropping the score column if you don't need it for training\n",
    "df = df[['sentence', 'label']]  # use this version instead if keeping it clean\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjlMx3uKn8_J"
   },
   "source": [
    "## Preparing Training/Validation/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZC5xBpyn1x6"
   },
   "outputs": [],
   "source": [
    "df_train, df_test, = train_test_split(df, stratify=df['label'], test_size=0.1, random_state=42)\n",
    "df_train, df_val = train_test_split(df_train, stratify=df_train['label'],test_size=0.1, random_state=42)\n",
    "print(df_train.shape, df_test.shape, df_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IP4qKnx3oLbU"
   },
   "source": [
    "## Load FinBERT Pre-trained Model\n",
    "The pretrained FinBERT model path on Huggingface is https://huggingface.co/yiyanghkust/finbert-pretrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fp0aotoEoGTp"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71VMqpWZoZfv"
   },
   "source": [
    "## Prepare Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyoYXFTRoYpH"
   },
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "dataset_train = dataset_train.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_val = dataset_val.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_test = dataset_test.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length' , max_length=128), batched=True)\n",
    "\n",
    "dataset_train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_val.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_test.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jTH1VW5odKr"
   },
   "source": [
    "## Define Training Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9uf9Aqqofwt"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy' : accuracy_score(predictions, labels)}\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir = 'temp/',\n",
    "        eval_strategy = 'epoch',\n",
    "        save_strategy = 'epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.005,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset_train,\n",
    "        eval_dataset=dataset_val,\n",
    "        compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXBHSn4RomuK"
   },
   "source": [
    "## Evaluate on Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PR4SZ0X3olVF"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "trainer.predict(dataset_test).metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dB9Olz34y5cF"
   },
   "outputs": [],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXitFyWooshx"
   },
   "source": [
    "## Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnq_3AYKomDF"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('finbert-sentiment-sec-press-releases/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oS2x94ef0K7"
   },
   "source": [
    "# Evaluate on All Press Releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O37fly6f0K7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dataframe with sentences grouped by original dataframe entry.\n",
    "`df_sentences` contains: unique_id (index), sentence\n",
    "'''\n",
    "# Essential imports - If required to run sole code block in the future\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load original dataframe with column 'text_pr' featured\n",
    "df = pd.read_parquet(\"sec.parquet\")               # In Google Colab, change to \"/content/sec.parquet\"\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"unique_id\"] = df.index  # give each press release a unique ID\n",
    "\n",
    "# Store sentence-level data\n",
    "sentence_records = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    uid = row[\"unique_id\"]\n",
    "    text = row[\"text_pr\"]\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        sentence_records.append({\n",
    "            \"unique_id\": uid,\n",
    "            \"sentence\": sent.text.strip()\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_sentences = pd.DataFrame(sentence_records)\n",
    "df_sentences.to_csv(\"sec_sentences_with_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjMroGJ9f0K7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Produces dataframe with sentences grouped by original dataframe entry, with the label and score for each sentence\n",
    "df_sentences contains: unique_id (index), sentence, label, score (confidence level that tuned FinBert has regarding its analysis)\n",
    "'''\n",
    "\n",
    "# Essential imports - If required to run sole code block in the future\n",
    "from transformers import pipeline, BertTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "model_path = \"./finbert-sentiment-sec-press-releases\"           # In Google Colab, change to \"/content/finbert-sentiment-sec-press-releases\"\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Load model and tokenizer\n",
    "sec_model = pipeline(\"text-classification\", model=model_path, tokenizer=tokenizer)\n",
    "\n",
    "# Loop through each sentence to generate a label and a confidence score\n",
    "results = []\n",
    "for sentence in tqdm(df_sentences[\"sentence\"], total=len(df_sentences)):\n",
    "    doc = sec_model(sentence)\n",
    "    label = doc[0]['label']\n",
    "    score = doc[0]['score']\n",
    "    results.append({\"label\": label, \"score\": score})\n",
    "\n",
    "# Add predictions into DataFrame\n",
    "df_sentences[[\"label\", \"score\"]] = pd.DataFrame(results, index=df_sentences.index)\n",
    "\n",
    "# Replace labels with descriptive adjectives\n",
    "df_sentences[\"label\"] = df_sentences[\"label\"].replace({\n",
    "    \"LABEL_0\": \"Neutral\",\n",
    "    \"LABEL_1\": \"Positive\",\n",
    "    \"LABEL_2\": \"Negative\"\n",
    "})\n",
    "\n",
    "# Save Generated DataFrame into CSV format\n",
    "df_sentences.to_csv(\"sec_sentences_with_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-c_Ug0NT078a"
   },
   "outputs": [],
   "source": [
    "# Make Labels Numerical, so we get an average score from all sentences in the same row of the dataframe\n",
    "df_sentences[\"numerical_score\"] = df_sentences[\"label\"].replace({\n",
    "    \"Neutral\": 0,\n",
    "    \"Positive\": 1,\n",
    "    \"Negative\": -1\n",
    "})\n",
    "\n",
    "'''\n",
    "df_average contains: \n",
    "- unique_id (index)\n",
    "- avg_score (the average of all numerical scores obtained from the sentences in the same row of the original dataframe)\n",
    "- avg_label (the sentiment label assigned based on the avg_score) \n",
    "'''\n",
    "\n",
    "# Compute the average numerical score per document\n",
    "df_average = (\n",
    "    df_sentences\n",
    "    .groupby(\"unique_id\", as_index=False)[\"numerical_score\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"numerical_score\": \"avg_score\"})\n",
    ")\n",
    "\n",
    "# Assign a sentiment label based on the averaged score\n",
    "def assign_label(x):\n",
    "    if x < 0:\n",
    "        return \"Negative\"\n",
    "    elif x == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "df_average[\"avg_label\"] = df_average[\"avg_score\"].apply(assign_label)\n",
    "\n",
    "# Inspect the result\n",
    "df_average.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final DataFrame: Appending original text_pr column to df_average\n",
    "- unique_id (index)\n",
    "- text_pr (the original press release text)\n",
    "- avg_score (the average of all numerical scores obtained from the sentences in the same row of the original dataframe)\n",
    "- avg_label (the sentiment label assigned based on the avg_score)\n",
    "'''\n",
    "\n",
    "deliverable = pd.DataFrame(\n",
    "    {\"unique_id\": df_average[\"unique_id\"],\n",
    "     \"text_pr\": df.loc[df_average[\"unique_id\"], \"text_pr\"].values,\n",
    "     \"avg_score\": df_average[\"avg_score\"],\n",
    "     \"avg_label\": df_average[\"avg_label\"]}\n",
    ")\n",
    "\n",
    "deliverable.to_csv(\"sec_press_releases_with_avg_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliverable.head()\n",
    "deliverable.to_parquet(\"sec_press_releases_with_avg_sentiment.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "finbert-sec-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
